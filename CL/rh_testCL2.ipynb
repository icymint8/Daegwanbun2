{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:14.748589Z",
     "start_time": "2024-12-06T15:40:13.289048Z"
    }
   },
   "source": [
    "# nltk 사용시 이 명령어 써줘야 함\n",
    "import nltk\n",
    "\n",
    "# 가상 환경 내에 설치될 경로 지정\n",
    "nltk.data.path.append('/venv/nltk_data')\n",
    "\n",
    "# 체크\n",
    "print(nltk.data.path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\rrhhk/nltk_data', 'C:\\\\Users\\\\rrhhk\\\\Desktop\\\\Codes\\\\Daegwanbun2\\\\venv\\\\nltk_data', 'C:\\\\Users\\\\rrhhk\\\\Desktop\\\\Codes\\\\Daegwanbun2\\\\venv\\\\share\\\\nltk_data', 'C:\\\\Users\\\\rrhhk\\\\Desktop\\\\Codes\\\\Daegwanbun2\\\\venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\rrhhk\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', '/venv/nltk_data']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:14.982928Z",
     "start_time": "2024-12-06T15:40:14.757097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.target_names"
   ],
   "id": "d9bcf2e191243bb9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:15.044688Z",
     "start_time": "2024-12-06T15:40:15.030022Z"
    }
   },
   "cell_type": "code",
   "source": "print(twenty_train.data[0])",
   "id": "cb941a8b5038edd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:15.107298Z",
     "start_time": "2024-12-06T15:40:15.092266Z"
    }
   },
   "cell_type": "code",
   "source": "twenty_train.target",
   "id": "c814d07b5d7f726a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:15.575365Z",
     "start_time": "2024-12-06T15:40:15.156868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ],
   "id": "734b0014e696b02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:16.058184Z",
     "start_time": "2024-12-06T15:40:15.623872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_vect_s = CountVectorizer(stop_words=\"english\")\n",
    "X_train_counts_s = count_vect_s.fit_transform(twenty_train.data)\n",
    "X_train_counts_s.shape"
   ],
   "id": "cf95489a79b2fd89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35482)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:17.461531Z",
     "start_time": "2024-12-06T15:40:16.115679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tdidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tdidf_transformer.fit_transform(X_train_counts.toarray())\n",
    "X_train_tfidf.shape"
   ],
   "id": "98360dfafbe3d8c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:40:17.539880Z",
     "start_time": "2024-12-06T15:40:17.519203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "docs_new = [\"cancer patient\", \"OpenGL on the GPU is fast\"]\n",
    "X_new_counts = count_vectorizer.transform(docs_new)\n",
    "X_new_tdidf = tdidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = nb.predict(X_new_tdidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(f\"{doc} => {twenty_train.target_names[category]}\")"
   ],
   "id": "4ccba0eb44bc40a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer patient => sci.med\n",
      "OpenGL on the GPU is fast => comp.graphics\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:14:17.223395Z",
     "start_time": "2024-12-06T16:14:16.786920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "categories = ['crime', 'technology', 'politics', 'science', 'entertainment','space']\n",
    "\n",
    "train_data = load_files(\n",
    "    container_path='text/train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    encoding='utf-8',\n",
    "    decode_error='replace',\n",
    "    random_state=42\n",
    ")\n",
    "X_train, y_train = train_data.data, train_data.target\n",
    "\n",
    "test_data = load_files(\n",
    "    container_path='text/test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    encoding='utf-8',\n",
    "    decode_error='replace',\n",
    "    random_state=42\n",
    ")\n",
    "X_test, y_test = test_data.data, test_data.target"
   ],
   "id": "b49c6fdbfa7075dd",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:37:29.077259Z",
     "start_time": "2024-12-06T16:37:15.033955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 3))),  # Converts text to BoW\n",
    "    ('tfidf', TfidfTransformer()),     # Converts BoW to TF-IDF\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=train_data.target_names))"
   ],
   "id": "ff5b48904a81c596",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        crime       0.72      0.99      0.83        89\n",
      "entertainment       1.00      0.08      0.15        61\n",
      "     politics       0.45      0.97      0.62        87\n",
      "      science       1.00      0.43      0.60        65\n",
      "        space       0.93      0.98      0.96        65\n",
      "   technology       0.93      0.24      0.38        59\n",
      "\n",
      "     accuracy                           0.66       426\n",
      "    macro avg       0.84      0.61      0.59       426\n",
      " weighted avg       0.81      0.66      0.61       426\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:10:44.546639Z",
     "start_time": "2024-12-06T17:10:37.508801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset using load_files\n",
    "dataset = load_files(\n",
    "    container_path='text/train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    encoding='utf-8',\n",
    "    decode_error='replace',\n",
    "    random_state=42\n",
    ")\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Function to analyze dataset\n",
    "def analyze_text_data(data):\n",
    "    # Total documents\n",
    "    num_docs = len(data)\n",
    "\n",
    "    # Total word count\n",
    "    total_words = sum(len(doc.split()) for doc in data)\n",
    "\n",
    "    # Vocabulary size using CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_vec = vectorizer.fit_transform(data)\n",
    "    vocab_size = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Type-to-Token Ratio (TTR)\n",
    "    unique_words = set(word for doc in data for word in doc.split())\n",
    "    ttr = len(unique_words) / total_words\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Number of Documents: {num_docs}\")\n",
    "    print(f\"Total Words: {total_words}\")\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"Type-to-Token Ratio (TTR): {ttr:.3f}\")\n",
    "\n",
    "    # Optional: Top 10 frequent words\n",
    "    word_counts = Counter(word for doc in data for word in doc.split())\n",
    "    print(\"Top 10 Frequent Words:\", word_counts.most_common(10))\n",
    "\n",
    "# Analyze the dataset\n",
    "analyze_text_data(X)"
   ],
   "id": "74bbc8f9af04005b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1722\n",
      "Total Words: 1261483\n",
      "Vocabulary Size: 36404\n",
      "Type-to-Token Ratio (TTR): 0.069\n",
      "Top 10 Frequent Words: [('the', 63417), ('to', 35838), ('and', 28691), ('of', 28667), ('a', 27034), ('in', 23691), ('that', 15370), ('for', 10610), ('was', 10558), ('on', 10429)]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:37:38.717462Z",
     "start_time": "2024-12-06T16:37:29.089776Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultinomialNB' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m feature_names \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mnamed_steps[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvectorizer\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mget_feature_names_out()\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Get feature importances from the DecisionTreeClassifier\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m importances \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_importances_\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Pair feature names with their importances\u001B[39;00m\n\u001B[0;32m     17\u001B[0m feature_importances \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(feature_names, importances))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'MultinomialNB' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "execution_count": 34,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the pipeline (from the earlier example)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Extract the trained DecisionTreeClassifier\n",
    "classifier = pipeline.named_steps['classifier']\n",
    "\n",
    "# Extract feature names from the vectorizer\n",
    "feature_names = pipeline.named_steps['vectorizer'].get_feature_names_out()\n",
    "\n",
    "# Get feature importances from the DecisionTreeClassifier\n",
    "importances = classifier.feature_importances_\n",
    "\n",
    "# Pair feature names with their importances\n",
    "feature_importances = list(zip(feature_names, importances))\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_features = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 20 features\n",
    "top_20_features = sorted_features[:20]\n",
    "\n",
    "# Prepare data for visualization\n",
    "top_20_names = [feature[0] for feature in top_20_features]\n",
    "top_20_values = [feature[1] for feature in top_20_features]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_20_names, top_20_values)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top 20 Feature Importances in Decision Tree\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ],
   "id": "6408e961b07ac85a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:21:16.126697Z",
     "start_time": "2024-12-06T17:18:13.146772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__max_df': [0.75, 1.0],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'classifier__alpha': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ],
   "id": "f284d559ecfd9780",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best Parameters: {'classifier__alpha': 0.1, 'vectorizer__max_df': 0.75, 'vectorizer__min_df': 2, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best Accuracy: 0.9111498257839722\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T17:55:45.348796Z",
     "start_time": "2024-12-06T17:46:46.865970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__max_df': [0.75, 1.0],\n",
    "    'vectorizer__min_df': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ],
   "id": "3feeadc9565d78b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Parameters: {'vectorizer__max_df': 0.75, 'vectorizer__min_df': 2, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best Accuracy: 0.9076655052264808\n"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
